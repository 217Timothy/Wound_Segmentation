import os
import sys
import json
import argparse
import numpy as np
import torch
import albumentations as A
from tqdm import tqdm

current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(current_dir)
sys.path.append(root_dir)

from src.models.unet import UNet
from src.datasets.wound_dataset import SegmentationDataset
from src.engine import infer_one_image as infer_one_image
from src.utils import load_checkpoint


# ==========================================
# 1. è¨­å®šåƒæ•¸èˆ‡åƒæ•¸è§£æå™¨
# ==========================================
IMAGE_SIZE = 512
RUN_NAME = "unet_v1" # é€™è£¡å¯ä»¥æ ¹æ“šéœ€è¦æ”¹æˆåƒæ•¸è¼¸å…¥ï¼Œç›®å‰å¯«æ­»ä¹Ÿå¯ä»¥


def get_args():
    parser = argparse.ArgumentParser(description="Inference on images using U-Net")
    
    # å¿…è¦åƒæ•¸
    parser.add_argument("--dataset", type=str, required=True,
                        help="è³‡æ–™é›†åç¨± (ä¾‹å¦‚ WoundSeg)")
    
    # è·¯å¾‘è¨­å®š
    parser.add_argument("--root", type=str, default="data/processed",
                        help="è³‡æ–™é›†æ ¹ç›®éŒ„")
    parser.add_argument("--split", type=str, default="val",
                        help="è¦è©•ä¼°çš„æ¸…å–® (val)")
    parser.add_argument("--checkpoint", type=str, default=f"checkpoints/{RUN_NAME}/best.pt",
                        help="æ¨¡å‹æ¬Šé‡è·¯å¾‘")
    parser.add_argument("--output", type=str, default=f"results/metrics/metrics_{RUN_NAME}.json",
                        help="è©•ä¼°å ±å‘Šè¼¸å‡ºè·¯å¾‘ (.json)")
    
    # å…¶ä»–
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu",
                        help="ä½¿ç”¨è¨­å‚™")
    parser.add_argument("--threshold", type=float, default=0.5,
                        help="äºŒå€¼åŒ–é–€æª»")
    
    return parser.parse_args()


def calculate_dice(pred, target):
    """
    è¨ˆç®—å–®å¼µåœ–çš„ Dice Score
    Args:
        pred: (H, W) 0/1 Numpy Array
        target: (H, W) 0/1 Numpy Array
    """
    
    intersection = (pred * target).sum()
    total = pred.sum() + target.sum()
    
    # å¦‚æœå…©å¼µåœ–éƒ½æ˜¯å…¨é»‘ (æ²’æœ‰å‚·å£)ï¼ŒDice æ‡‰è©²æ˜¯ 1.0 (æ»¿åˆ†)
    if total == 0:
        return 1.0
    
    return (2. * intersection) / (total + 1e-6)


def main():
    args = get_args()
    
    print(f"[INFO] Dataset:    {args.dataset}")
    print(f"[INFO] Split:      {args.split}")
    print(f"[INFO] Checkpoint: {args.checkpoint}")
    print(f"[INFO] Device:     {args.device}")
    
    # 1. è¼‰å…¥æ¨¡å‹
    if not os.path.exists(args.checkpoint):
        print(f"[Error] Checkpoint not found: {args.checkpoint}")
        return

    print("[INFO] Loading model...")
    model = UNet(n_channels=3, n_classes=1).to(args.device)
    load_checkpoint(args.checkpoint, model)
    
    # 2. æº–å‚™ Dataset
    # é€™è£¡æˆ‘å€‘åªçµ¦ Resizeï¼Œå‰©ä¸‹çš„ Manual Normalization äº¤çµ¦ Dataset å…§éƒ¨è™•ç†
    transform = A.Compose([
        A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),
    ])
    
    dataset = SegmentationDataset(
        root_dir=args.root,
        datasets=[args.dataset],
        split=args.split,
        transform=transform
    )
    
    if len(dataset) == 0:
        print(f"[Error] No images found for {args.dataset} ({args.split})")
        return
    
    # 3. é–‹å§‹è©•ä¼°è¿´åœˆ
    print(f"[INFO] Evaluating on {len(dataset)} images...")
    dice_scores = []
    
    # é€™è£¡æˆ‘å€‘ä¸ä½¿ç”¨ DataLoaderï¼Œç›´æ¥ç”¨ index å­˜å–ï¼Œç¢ºä¿ä¸€å¼µä¸€å¼µç®—
    for i in tqdm(range(len(dataset))):
        img_tensor, mask_tensor = dataset[i]
        
        # A. æ¨è«– (Prediction)
        pred_mask = infer_one_image(
            model,
            img_tensor,
            args.device,
            args.threshold
        )
        
        # B. è™•ç†æ¨™æº–ç­”æ¡ˆ (Ground Truth)
        # æŠŠ Tensor è½‰æˆ Numpy (H, W)ï¼Œä¸¦ç¢ºä¿å®ƒæ˜¯æ•´æ•¸ 0/1
        gt_mask = mask_tensor.squeeze().numpy().astype(np.uint8)
        
        # C. ç®—åˆ†
        score = calculate_dice(pred_mask, gt_mask)
        dice_scores.append(score)
    
    mean_dice = np.mean(dice_scores)
    std_dice = np.std(dice_scores)
    
    print(f"\nğŸ“Š Evaluation Results")
    print(f"   Dataset:   {args.dataset}")
    print(f"   Mean Dice: {mean_dice:.4f}")
    print(f"   Std Dev:   {std_dice:.4f}")
    
    os.makedirs(args.out, exist_ok=True)
    
    report = {
        "dataset": args.dataset,
        "split": args.split,
        "checkpoint": args.checkpoint,
        "mean_dice": float(mean_dice),
        "std_dice": float(std_dice),
        "num_samples": len(dataset),
        "scores_detail": [float(s) for s in dice_scores] # å­˜ä¸‹æ¯ä¸€å¼µçš„åˆ†æ•¸
    }
    
    with open(args.out, "w") as f:
        json.dump(report, f, indent=4)
        
    print(f"âœ… Report saved to {args.out}")


if __name__ == "__main__":
    main()