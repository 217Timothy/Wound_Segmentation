import os
import cv2
import torch
import numpy as np
from tqdm import tqdm
from torch.utils.data import Dataset

class SegmentationDataset(Dataset):
    
    def __init__(
        self, 
        root_dir: str, 
        datasets, 
        split="train",
        transform=None,
        cache_data=True
    ):
        """
        é€šç”¨åˆ†å‰²è³‡æ–™é›†
        Args:
            root_dir (str): 'data/processed'
            datasets (list): è³‡æ–™é›†åç¨±åˆ—è¡¨ï¼Œä¾‹å¦‚ ['WoundSeg', 'CO2Wound']
            split (str): 'train', 'val', æˆ– 'test'
            transform (albumentations): è³‡æ–™å¢å¼·ç‰©ä»¶
            cache_data (bool): æ˜¯å¦å°‡è³‡æ–™é å…ˆè¼‰å…¥ RAM (åŠ é€Ÿç”¨)
        """
        
        self.root_dir = root_dir
        self.transform = transform
        self.cache_data = cache_data
        self.files = [] # é€™æ˜¯å­˜æ”¾æ‰€æœ‰æª”æ¡ˆè·¯å¾‘çš„å¤§æ¸…å–®
        
        # --- å­˜æ”¾å¿«å–è³‡æ–™çš„åˆ—è¡¨ ---
        self.cached_images = []
        self.cached_masks = []
        
        # 1. è’é›†æª”æ¡ˆè·¯å¾‘
        for ds in datasets:
            # a. è®€å– preprocess ç”Ÿæˆçš„ txt æ¸…å–®
            # è·¯å¾‘ç¯„ä¾‹: data/processed/splits/WoundSeg/train.txt
            split_file = os.path.join(self.root_dir, "splits", ds, f"{split}.txt")
            
            if not os.path.exists(split_file):
                print(f"[Warn] Split file not found: {split_file} (Skipping {ds})")
                continue
            
            with open(split_file, "r") as f:
                fnames = [line.strip() for line in f.readlines()]
            
            # b. çµ„åˆå®Œæ•´è·¯å¾‘
            # è³‡æ–™å¯¦éš›ä½ç½®: data/processed/WoundSeg/train/images/WS_001.png
            base_path = os.path.join(self.root_dir, ds, split)
            for fname in fnames:
                img_path = os.path.join(base_path, "images", fname)
                mask_path = os.path.join(base_path, "masks", fname)
                self.files.append((img_path, mask_path))
        
        # 2. ğŸ”¥ é å…ˆè¼‰å…¥ RAM
        if self.cache_data:
            print(f"[INFO] Caching {len(self.files)} images into RAM for '{split}' set...")
            for img_path, mask_path in tqdm(self.files, desc=f"Loading {split}"):
                img = cv2.imread(img_path)
                if img is None:
                    print(f"âŒ [Error] Skip bad image: {img_path}")
                    continue
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                
                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
                if mask is None:
                    print(f"âŒ [Error] Skip bad mask: {mask_path}")
                    continue
                
                self.cached_images.append(img)
                self.cached_masks.append(mask)
            
            if len(self.cached_images) != len(self.files):
                print(f"[WARN] Some images failed to load. Resizing dataset from {len(self.files)} to {len(self.cached_images)}")
                # é€™è£¡ç°¡å–®è™•ç†ï¼šç›´æ¥ä¾è³´ cached_images çš„é•·åº¦
                # ç‚ºäº†é¿å… index errorï¼Œæˆ‘å€‘æŠŠ self.files æ¸…ç©ºæˆ–ä¸ç”¨å®ƒäº†
                pass
    
    
    def __len__(self):
        return len(self.files)
    
    
    def __getitem__(self, idx):
        
        # 1. æ‹¿è·¯å¾‘
        img_path, mask_path = self.files[idx]
        
        # 2. è®€åœ–ç‰‡ (è½‰ RGB)
        img = cv2.imread(img_path)
        # ğŸ”¥ [é™¤éŒ¯é—œéµ] æª¢æŸ¥æ˜¯å¦è®€å–å¤±æ•—
        if img is None:
            raise FileNotFoundError(f"âŒ ç„¡æ³•è®€å–åœ–ç‰‡ï¼Œè«‹æª¢æŸ¥è·¯å¾‘æ˜¯å¦å­˜åœ¨: {img_path}")
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        
        # 3. è®€ Mask (è½‰å–®å±¤ç°éš)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        if mask is None:
            raise RuntimeError(f"Failed to read mask: {mask_path}")
        
        # 4. ğŸ”¥ Data Augmentation (åœ¨é€™è£¡åš!)
        # æˆ‘å€‘ä½¿ç”¨ albumentationsï¼Œå®ƒæœƒåŒæ™‚è™•ç† image å’Œ mask
        if self.transform is not None:
            augmented = self.transform(image=img, mask=mask)
            img = augmented["image"]
            mask = augmented["mask"]
        
        # ================= ã€æœ€å¾ŒåŠ ä¸Šé€™å…©å±¤é˜²è­·ç¶²ã€‘ =================
        
        # é˜²è­·ç¶² 1ï¼šæŠŠ 0 å’Œ 255 å¼·åˆ¶è½‰æˆ 0.0 å’Œ 1.0 çš„æµ®é»æ•¸ (Float)
        if isinstance(mask, torch.Tensor):
            mask = (mask > 0).float()
        else:
            # é é˜²è¬ä¸€ transform æ²’æœ‰ ToTensorV2 å°è‡´å›å‚³ numpy çš„æƒ…æ³
            mask = torch.from_numpy((mask > 0).astype(np.float32))
            
        # é˜²è­·ç¶² 2ï¼šè£œä¸Š Channel ç¶­åº¦ ([512, 512] -> [1, 512, 512])
        if len(mask.shape) == 2:
            mask = mask.unsqueeze(0)

        return img, mask